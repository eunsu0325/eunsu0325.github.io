---
layout: single
title:  "SupConLoss vs Difficulty-Aware SupConLoss"
categories: 논문
tags: [논문]
comments: true
author_profile: false
toc: true   
math: true
sidebar:        
    nav: "counts"
---

# SupConLoss vs. Difficulty-Aware SupConLoss 표준보

---

## 1. 포가 설명: SupConLoss의 기본 구조

\*\*Supervised Contrastive Loss (SupConLoss)\*\*는 같은 클래스의 임베딩들은 가깝게, 다른 클래스는 멀어지도록 학습하는 contrastive learning의 대표적 손실 함수입니다. SupConLoss는 cross-entropy 기반 분류 손실이 가지는 클래스 간 경계 기반 학습의 한계를 극복하기 위해 제안되었습니다.

### 🔹 수식

$$
\mathcal{L}_{\text{SupCon}} = \sum_{i=1}^{N} \frac{-1}{|P(i)|} \sum_{p \in P(i)} \log \frac{\exp(\mathbf{z}_i \cdot \mathbf{z}_p / \tau)}{\sum_{a \in A(i)} \exp(\mathbf{z}_i \cdot \mathbf{z}_a / \tau)}
$$

* $$\mathbf{z}_i$$: anchor의 임베딩 벡터
* $$P(i)$$: anchor $i$와 같은 클래스의 positive 인덱스 집합
* $$A(i)$$: anchor를 제외한 모든 나머지 샘플 인덱스
* $$\tau$$: temperature scaling factor

이 손실 함수는 anchor 벡터가 같은 클래스의 샘플과는 더 높은 유사도를 갖도록 학습하고, 모든 다른 샘플과의 유사도는 상대적으로 낮아지도록 유도합니다.

### 🔍 수식 구조 해설

* 분자는 anchor와 positive 샘플 간의 cosine similarity를 temperature로 스케일링한 뒤 exponential을 취한 값입니다.
* 분모는 anchor와 배치 내의 모든 다른 샘플(positive + negative)과의 similarity 전체에 대해 exponential을 적용한 값의 합입니다.
* 이 구조는 softmax 형태를 따르며, positive 쌍의 유사도를 높이고 negative 쌍과의 구별을 강화합니다.
* positive similarity가 클수록 log 값이 커지고 loss는 작아져 학습이 잘 되었다는 신호를 제공합니다.
* 핵심적으로, 이 손실은 anchor-positive 관계를 강조하면서도 배치 전체에서 상대적인 분포를 고려합니다.

---

## 2. SupConLoss의 한계

### ❗ 1) Positive 쌍에 동일한 중요도 부여

* SupConLoss는 모든 positive 쌍에 동일한 학습 비중(가중치 1)을 부여합니다. 하지만 실제 데이터에서는 같은 클래스라도 조명, 각도, 노이즈 등의 영향으로 유사도가 낮은 hard positive가 존재합니다.
* 이러한 hard positive는 학습이 더 필요한 대상이지만, 동일한 비중으로 처리되기 때문에 충분한 보정 학습이 이루어지지 않습니다.

### ❗ 2) Hard Negative에 대한 패널티 미비

* 유사도가 높아 혼동되기 쉬운 hard negative에 대해 별도의 패널티가 존재하지 않기 때문에, inter-class confusion이 발생할 수 있으며 이는 오인식률 증가로 이어집니다.

### ❗ 3) 도메인 변화(Open-set, Cross-domain)에 취약

* 같은 클래스라도 도메인이 달라지면 임베딩 분포가 달라져 유사도가 떨어지고, 이는 SupCon 구조에서 학습되지 않은 환경에 대한 일반화 성능 저하로 이어집니다.

---

## 3. 🔧 난이도 기반 SupConLoss의 설계 의도

### 🎯 목적

* Hard Positive(유사도 낮은 같은 클래스 쌍)에 더 큰 학습 신호를 부여
* Hard Negative(유사도 높은 다른 클래스 쌍)에 더 강한 패널티 부여
* 손실 함수 자체에 학습 난이도 정보를 반영하여, 학습이 필요한 샘플에 집중하는 효과를 유도함

이러한 구조는 정적인 손실 함수 구조에서 벗어나, 학습 난이도를 기반으로 동적인 손실 조정을 가능하게 합니다.

---

## 4. 📐 수식 기반 개선 구조

### 🔹 Hard Positive 가중치

$$w_{ij}^{+} = 1 + \alpha \cdot \left( \frac{s_{th}^{+} - \text{sim}_{ij}}{s_{th}^{+}} \right)$$

* $$\text{sim}_{ij}$$: anchor $z_i$와 positive 샘플 $$z_j$$ 사이의 cosine similarity
* $$s_{th}^{+}$$: hard positive로 판단할 기준 유사도 임계값
* $$\alpha$$: 가중치 scaling 계수

→ 유사도가 낮을수록 $\text{sim}_{ij}$가 작아지므로 전체 가중치 $w_{ij}^{+}$는 커집니다. 즉, 같은 클래스라도 유사도가 낮은 쌍에 대해 더 큰 학습 비중이 부여되어, difficult positive sample에 집중적인 gradient가 흐르도록 유도합니다.

### 🔹 Hard Negative penalty

$$w_{ij}^{-} = 1 + \beta \cdot \left( \frac{\text{sim}_{ij} - s_{th}^{-}}{1 - s_{th}^{-}} \right)$$

* $$s_{th}^{-}$$: hard negative로 판단할 similarity 임계값
* $$\beta$$: hard negative penalty 증가 계수

→ 유사도가 높은 다른 클래스 샘플의 경우 $\text{sim}_{ij}$$가 커지므로 penalty 가중치 $$w_{ij}^{-}$$도 커지게 됩니다. 이는 class confusion 가능성이 높은 샘플에 대해 분리 학습을 강화하는 효과를 냅니다.

### 🔍 수식 구조 해설

* 두 가중치 식 모두 similarity 값을 연속적으로 해석하여 샘플 간 학습 난이도에 따라 gradient 흐름을 조절합니다.
* hard positive일수록 학습에 더 집중되며, hard negative일수록 분리 강화를 유도합니다.
* 기존 SupConLoss의 한계였던 sample difficulty 무시 문제를 효과적으로 보완합니다.

### 🔹 최종 손실 구조

$$
\mathcal{L}_{\text{DA-SupCon}} = \sum_{i=1}^{N} \sum_{j \neq i} w_{ij} \cdot 
\left[ - \mathbb{I}(y_i = y_j) \cdot \log 
\frac{\exp(\text{sim}_{ij}/\tau)}{\sum_k \exp(\text{sim}_{ik}/\tau)} \right] 
$$

* $$w_{ij}$$: 난이도 기반 가중치 (hard positive/negative 여부에 따라 $$w_{ij}^{+}$$ 또는 $$w_{ij}^{-}$$로 결정)
* $$\mathbb{I}(y_i = y_j)$$: 클래스 동일 여부 indicator
* 기존 SupCon과 동일한 softmax 기반 구조를 유지하되, 샘플별 난이도 기반으로 loss weighting을 수행하여 학습 방향을 동적으로 조절합니다.

---

## 5. 🧪 실험 결과 및 기대 효과

* 스마트폰 도메인(Open-set, Cross-domain) 환경에서:

  * **EER**: 7.56%
  * **AUC**: 0.982

### ✅ 효과 요약

* Hard positive/negative 쌍에 학습 자원을 집중시킴으로써 모델 일반화 성능 향상
* 동일 사용자 간 임베딩은 더 응집되게, 다른 사용자 간 임베딩은 더 멀게 분리됨
* Open-set 상황에서도 안정적인 성능 유지
* 실제 시스템에서의 실시간성 확보를 위해 Faiss 최적화를 적용하여 연산 병목도 해결함

---

## 6. 📊 요약 비교표

| 항목               | 기존 SupConLoss   | 난이도 기반 SupConLoss    |
| ---------------- | --------------- | -------------------- |
| Positive 가중치     | 고정값 1           | 유사도 기반 동적 조정         |
| Negative penalty | 없음              | 유사도 기반 penalty 부여    |
| 학습 집중 대상         | 모든 쌍 동일         | hard sample 집중       |
| 도메인 적응력          | 약함              | 강함 (Open-set 대응)     |
| 연산 최적화           | 미적용             | Faiss 최적화로 3배 이상 가속  |
| 실험 결과            | 도메인 전이 시 오인식 발생 | EER 7.56%, AUC 0.982 |

---

## ✅ 결론

기존 SupConLoss는 효과적인 contrastive learning을 제공하지만, 실제 환경에서의 복잡한 변화와 하드 샘플에 대한 민감한 대응이 부족한 구조입니다. 이에 비해 난이도 기반 SupConLoss는 학습 난이도를 반영하여, 더 효율적이고 일반화 성능이 높은 인식 모델을 학습할 수 있게 합니다. 특히 Open-set 및 Cross-domain 환경에서의 강건한 인식 성능 확보에 적합하며, 실시간 시스템을 고려한 연산 최적화까지 함께 설계되어 실제 응용 가능성이 높습니다.
