---
layout: single
title:  "AI Research Engineer를 위한 CNN 이해하기(1)"
categories: ML
tags: [Pytorch, CNN]
comments: true
author_profile: false
toc: true   
sidebar:    
    nav: "docs"
---

# Convolution의 이해


## Pytorch - Convolution 구현

아래는 Pytorch로 Convolution 간단하게 구현한 코드입니다.

```python   
import torch
import torch.nn as nn
import numpy as np
from PIL import Image

class Conv2DGray(nn.Module):
    def __init__(self, kernel):
        super(Conv2DGray, self).__init__()
        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=kernel.shape, bias=False)
        with torch.no_grad():
            self.conv.weight = nn.Parameter(torch.tensor(kernel, dtype=torch.float32).unsqueeze(0).unsqueeze(0))

    def forward(self, image: Image.Image):
        image_tensor = self.pre_proc(image)
        output_tensor = self.conv(image_tensor)
        output_image = self.post_proc(output_tensor)
        return output_image
    
    def pre_proc(self, gray_image: Image.Image) -> torch.Tensor:
        image_np = np.array(gray_image).astype(np.float32)
        image_tensor = torch.tensor(image_np).unsqueeze(0).unsqueeze(0)
        return image_tensor

    def post_proc(self, image_tensor: torch.Tensor) -> Image.Image:
        output_image = image_tensor.squeeze(0).squeeze(0).detach().numpy()
        return Image.fromarray(output_image.astype(np.uint8))
```


### 동작 순서
**초기화 (__init__)**<br>
사용자 정의 2D 커널(필터)를 받아 Conv2D 레이어의 가중치로 등록합니다.<br>
torch.no_grad() 블록을 사용해 학습되지 않는 고정된 필터로 설정합니다.<br><br>
**전처리** (pre_proc)<br>
PIL 이미지를 NumPy 배열로 변환하고,<br>
PyTorch 텐서(4D: batch, channel, height, width)로 변환합니다.<br><br>
**순전파**(forward)<br>
입력 이미지를 컨볼루션 레이어에 전달하고,<br>
그 결과를 후처리하여 PIL 이미지로 반환합니다.<br><br>
**후처리** (post_proc)<br>
텐서를 다시 NumPy 배열로 변환하고,<br>
8비트 정수형 이미지로 변환해 반환합니다.<br>

### <상세분석> 1. nn.Module 상속
```python   
class Conv2DGray(nn.Module)
```
파이썬 클래스 Conv2DGray를 정의하고, PyTorch 신경망 모듈(nn.Module)을 상속받아
"특정 연산(여기서는 Conv2D)을 수행하는 커스텀 레이어"를 만든 것.<br>
이렇게 하면 PyTorch 모델로 등록되어, 학습(가중치 업데이트), 추론(순전파), GPU 연산 등을 쉽게 처리할 수 있다.

💡 **클래스와 상속이 궁금하다면?**  
[📖 객체지향 프로그래밍과 PyTorch 상속 이해하기](https://eunsu0325.github.io/oop/oop/){: .btn .btn--info}

### <상세분석> 2. 초기화 (__init__)
```python   
 def __init__(self, kernel):
```  

메서드 이름: __init__ (생성자)<br>
매개변수: kernel<br>
2D 형태의 커널(필터)을 받아서, 컨볼루션 레이어의 가중치로 설정할 예정.<br>
이 클래스(Conv2DGray)가 생성될 때 자동으로 호출돼서, 내부 변수를 초기화하는 과정.<br>
즉, Conv2DGray(kernel)처럼 객체를 생성하면, 커널을 전달받아 초기 세팅을 하게 됨.

```python   
super(Conv2DGray, self).__init__()
```
부모 클래스: nn.Module
Conv2DGray는 nn.Module을 상속받았기 때문에, 부모 클래스의 생성자를 명시적으로 불러줘야 함.<br>
이유: PyTorch 내부적으로 nn.Module이 가지고 있는 가중치 관리, 그래디언트 추적 등 기능을 올바르게 초기화해주기 위해서.<br>
"부모(nn.Module)를 Conv2DGray 객체 관점에서 초기화해줘!"라는 뜻이야.<br>

```python   
self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=kernel.shape, bias=False)
```
이 줄은 PyTorch의 2D 컨볼루션 레이어를 초기화하는 코드야.


in_channels=1<br>
입력 채널이 1개라는 뜻 → 그레이스케일 이미지를 염두에 둔 설정.<br>

out_channels=1:<br>
출력 채널도 1 → 결과도 **1채널(그레이스케일)**로 출력.<br>

kernel_size=kernel.shape:<br>
kernel이 2D 배열(예: 3x3, 5x5 등)일 것이므로, 그 가로×세로 크기를 그대로 사용.<br>

bias=False:<br>
편향(bias)을 사용하지 않음.<br>
주로 엣지 검출 같은 고정된 필터에는 편향이 필요 없어서 빼는 경우가 많음.

> 1. 입력 채널(Input Channels)
>
>그레이스케일 이미지: 채널=1 (검은색~흰색 농도 정보만 있음)<br>
>컬러(RGB) 이미지: 채널=3 (Red, Green, Blue 3가지 색상 채널)<br>
>추가 채널: 알파 채널(RGBA)이 있으면 채널이 4가 될 수도 있고,<br>
>Depth 정보나 다른 센서 데이터가 붙으면 채널이 더 커질 수도 있어.<br>
>결국 입력 채널은 **"이 레이어가 한 번에 처리해야 할 데이터 깊이"**라고 할 수 있어.<br>
>
> 2. 출력 채널(Output Channels)
>
>합성곱 레이어는 **필터(커널)**를 여러 개 사용해.<br>
>각 필터는 입력을 받아 새로운 **특징맵(feature map)**을 만든다고 볼 수 있어.<br>
>필터 개수 = 출력 채널 수라고 생각하면 돼.

```python   
self.conv.weight = nn.Parameter(torch.tensor(kernel, dtype=torch.float32).unsqueeze(0).unsqueeze(0))
```

이 줄이 실질적으로 "사용자 정의 커널"을 Conv 레이어의 가중치로 직접 넣어주는 핵심이야.<br>
torch.tensor(kernel, dtype=torch.float32)<br>
-> 넘파이 배열(kernel)을 PyTorch 텐서로 변환.<br>
dtype=float32로 설정해, GPU/CPU 계산에서 표준으로 많이 사용하는 부동소수 형태.<br>
<br>
.unsqueeze(0).unsqueeze(0)
텐서의 차원을 2번 확장.<br>
보통 kernel.shape == (KH, KW) (예: 3x3)일 텐데,<br>
첫 번째 unsqueeze(0) → (1, KH, KW) (필터 개수 차원)<br>
두 번째 unsqueeze(0) → (1, 1, KH, KW) (입력 채널 차원까지)<br>
Conv2d 가중치는 (out_channels, in_channels, kernel_height, kernel_width) 형태로 되어 있어야 해.
여기서는 **out_channels=1, in_channels=1**이므로, 결국 (1, 1, KH, KW)가 맞는 형태가 되는 거지.

nn.Parameter(...)<br>
nn.Parameter**는 PyTorch에서 **학습 가능한 가중치(파라미터)**로 등록하기 위한 특별한 래퍼(wrapper) 역할을 해요.<br>
일반 텐서(torch.Tensor)를 **nn.Parameter**로 감싸 주면, 모델(nn.Module) 안에서 이 텐서를 학습 대상으로 인식하게 돼요.<br>
이렇게 만든 텐서를 **PyTorch가 학습/추적할 수 있는 "파라미터"**로 등록.<br>
하지만 with torch.no_grad(): 블록 안이니까, 실제 학습(optimizer 업데이트)은 되지 않는 고정 필터가 될 거야.<br>

self.conv.weight = ...<br>
최종적으로 Conv2D 레이어의 weight에 우리가 만든 텐서를 대입.<br>
그 결과, 합성곱을 수행할 때 PyTorch가 이 커널을 사용해 이미지에 연산을 적용하게 됨.

## `forward` 메서드 분석

아래 코드는 PyTorch 신경망 모듈에서 가장 중요한 메서드인 `forward`를 정의한 부분입니다.
이 메서드를 통해 **실제 연산**(컨볼루션 필터링)이 어떻게 이루어지는지 살펴볼 수 있어요.

```python
def forward(self, image: Image.Image):
    image_tensor = self.pre_proc(image)
    output_tensor = self.conv(image_tensor)
    output_image = self.post_proc(output_tensor)
    return output_image

함수 시그니처
def forward(self, image: Image.Image):<br>
image: Image.Image:<br>
입력으로 PIL 이미지(그레이스케일)를 받는다고 명시하고 있어요.<br>

image_tensor = self.pre_proc(image)<br>
이 단계에서 PIL 이미지를 PyTorch 텐서로 변환하는 작업이 이뤄져요.<br>
내부적으로 pre_proc 메서드가 다음 과정을 수행합니다.<br>
PIL 이미지를 NumPy 배열로 (np.array).<br>
NumPy 배열을 PyTorch 텐서(torch.tensor)로.
텐서 형태를 (배치, 채널, 높이, 너비), 즉 (1, 1, H, W)로 맞춤.
output_tensor = self.conv(image_tensor)
핵심 연산: 컨볼루션 레이어(nn.Conv2d)에 텐서를 입력.
이 시점에 **사용자가 정의한 커널(가중치)**가 적용돼서,
이미지에 **합성곱(Convolution)**이 수행됨.
결과로, 컨볼루션 결과가 담긴 4D 텐서((1, 1, H_out, W_out))가 생성돼요.
output_image = self.post_proc(output_tensor)
컨볼루션 결과인 텐서를 다시 이미지 형태로 변환하는 과정.
내부적으로 post_proc 메서드가 다음을 수행합니다:
텐서의 불필요한 차원(배치, 채널)을 squeeze로 제거.
detach().numpy()로 NumPy 배열로 변환.
Image.fromarray(..., mode='L') 형태로 PIL 이미지로 복원.
return output_image
최종적으로 PIL 이미지를 반환.
이 이미지는 합성곱 필터가 적용된 결과물이므로, 엣지 검출이나 블러 같은 효과를 확인할 수 있어요.


👉 [nn.Conv2d 내부 동작 완벽 분석](https://yourblog.com/nn-conv2d-explained)  
[객체지향프로그밍의 표현](https://yourblog.com/nn-conv2d-explained){: .btn .btn--info}
